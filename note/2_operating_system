4.Concurrency and Threads.	Many hands make light work.

-----------------------------------------------------------------------------------------------
Thread Use Cases. What are threads useful for?
In a program, we can represent each concurrent task as a thread. Each thread provides the 
abstraction of sequential execution similar to the traditional programming model. In fact, we
can think of a traditional program as single-threaded with one logical sequence of steps as
each instruction follows the previous one. Each individual thread follows a single sequence of
steps as it executes statements, iterates through loops, calls/returns from procedures.

Four Reasons to Use Threads
	Program structure: expressing logically concurrent tasks.
	Responsiveness: shifting work to run in the background.
					Case:when writing a file, the operating system stores the modified
						 data in a kernel buffer, and returns immediately to the application.
						 In the background, the operating system kernel runs a separate thread
						 to flush the modified data out to disk.
	Performance: exploiting multiple processors.
				 An advantage to using threads for parallelism is that the number of threads
				 need not exactly match the number of processors in the hardware on which it
				 is runing. The operating system transparently switches which threads run on
				 which processors.
	Performance: managing I/O devices.
-----------------------------------------------------------------------------------------------

-----------------------------------------------------------------------------------------------
Thread Abstraction. What is the thread abstraction as seen by a programmer?

A thread is a single execution sequence that represents a separately schedulable task.
	a.Single execution sequence. Each thread executes a sequence of instructions just
	as in the familiar sequential programming model.
	b.Separately schedulable task. The operating system can run, suspend, or resume a thread at
	any time.

Running,Suspending,and Resuming Threads

Why "Unpredictable Speed"?
	We must coordinate thread actions through explicit synchronization rather than by trying
	to reason about their relative speed.

Is a kernel interrupt handler a thread?
No, an interrupt handler is not a thread. A kernel interrupt handler shares some resemblance
to a thread: it is a single sequence of instructions that executes from beginning to end. 
However, an interrupt handler is not independently schedulable: it is triggered by a hardware
I/O event, rather than a decision by the thread scheduler in the kernel. Once started, the
interrupt handler runs to completion, unless preempted by another interrupt.
-----------------------------------------------------------------------------------------------

-----------------------------------------------------------------------------------------------
Simple Thread API. How can programmers use threads?

void thread_create(thread,func,arg)
void pthread_create(thread,NULL,func,(void *)arg)
	Create a new thread, storing information about it in thread. Concurrently with the calling
	thread, thread executes the function func with the argument arg.

void thread_yield()
	The calling thread voluntarily gives up the processor to let some other thread(s) run. The
	scheduler can resume running the calling thread whenever it chooses to do so.

int thread_join(thread)
void pthread_join(thread,(void **)exitValue)
	Wait for thread to finish if it has not already done so; then return the value passed to
	thread_exit by that thread. Note that thread_join may be called only once for each thread.

void thread_exit(ret)
	Finish the current thread. Store the value ret in the current thread's data structure.
	If another thread is already waiting in a call to thread_join, resume it.

Creating and scheduling threads are separate operations. Although threads are usually scheduled
in the order that they are created, there is no guarantee. Further, even if thread 2 started
running before thread 5, it might be preempted before it reaches the printf call.

Why must the "Thread returned" message from thread 2 print before the Thread returned message
from thread 5?
Since the threads run on virtual processors with unpredictable speeds, the order in which the
threads finish is indeterminate. However, the main thread checks for thread completion in the
order they were created. It calls thread_join for thread i+1 only after thread_join for thread
i has return.

In practice, the operating system will often create a thread to run blockzero in the
background. The memory of an exiting process does not need to be cleared until the memory is
needed -- that is, when the next process is created.
To exploit this flexibility, the operating system can create a set of low priority threads to
run blockzero. The kernel can then return immediately and resume running application code.
Later on, when the memory is needed, the kernel can call thread_join. If the zero is complete
by that point, the join will return immediately; otherwise, it will wait until the memory is
safe to use.
-----------------------------------------------------------------------------------------------

-----------------------------------------------------------------------------------------------
Thread Data Structures and Life Cycle. 
What data structures does the operating system use to manage threads?

The operating system provides the illusion that each thread runs on its own virtual processor
by transparently suspending and resuming threads. For the illusion to work, the operating
system must precisely save and restore the state of a thread. However, because threads run
either in a process or in the kernel, there is also shared state that is not saved or restored
when switching the processor between threads.

 Shared State		   Thread 1's
					Per-Thread State
 ------------		|---------------|
|	 Code	 |		| Thread Control|
 ------------		|  Block(TCB)   |
					|---------------|
					|	  Stack		|
 ------------		|   Information	|
|	Global	 |		|---------------|
|  Varivbles |		|	  Saved		|
 ------------		|	Registers	|
					|---------------|
 ------------		|	  Thread	|
|	 Heap	 |		|    Metadata	|
 ------------		|---------------|
 

					|	  Stack 	|
					|...............|
					|...............|
					|---------------|

The thread control block stores the per-thread state:
	The current state of the thread's computation(e.g.,saved proecessor registers and a 
	pointer to the stack)
	and metadata needed to manage the thread(e.g.,the thread's ID,scheduling prority,owner,and
	resource consumption). 
Shared state includes the program's code, global static varibles, and the heap.

Per-Thread State and Thread Control Block(TCB)
	For every thread the operating system creates,it creates one TCB.
	The thread control block holds two types of per-thread information:
		1.The state of the computation being performed by the thread.
		2.Metadata about the thread that is used to manage the thread.

Per-thread Computation State.
	a pointer to the thread's stack and a copy of its processor registers.

	Stack: A thread's stack is the same as the stack for a single-threaded computation -- it
	stores information needed by the nested procedures the thread is currently running.
	When a new thread is created, the operating system allocates it a new stack and
	stores a pointer to that stack in the thread's TCB.

	Copy of processor registers: A processor's registers include not only its general-purpose
	registers for storing intermediate values for ongoing computations, but they also include
	special-purpose registers, such as the instruction pointer and stack pointer.
	In some systems, the general-purpose registers for a stopped thread are stored on the top
	of the stack, and the TCB contains only a pointer to the stack.In other systems, the TCB
	contains space for a copy of all processor registers.

Per-thread Metadata.
	The TCB also includes per-thread metadata -- information for managing the thread. For
	example, each thread might have a thread ID, scheduling priority, and status (e.g,
	whether the thread is waiting for an event or is ready to be placed onto a processor).

Shared State
	program code is shared by all threads in a process, although each thread may be executing
	at a different place within that code. Additionally, statically allocated global variables
	and dynamically allocated heap variables can store information that is accessible to all
	threads.

In addition to the per-thread state that corresponds to execution state in the single-threaded
case,some systems include additional thread-local variables.These variables are similar to
global variables in that their scope spans different procedures, but they differ in that each
thread has its own copy of these variables.
Errno. In a multi-threaded program,however,multiple threads can perform system calls
concurrently.Rather than redefine the entire UNIX system call interface for a multi-threaded
environment, errno is now a macro that maps to a thread-local variable containing the error
code for that thread's most recent system call.
Heap internals. for performance reasons heaps may internally subdivide their space int 
per-thread regions.The advantage of subdividing the heap is that multiple threads can each
allocate objects at the same time without interfering with one anoter.Further, by allocating
objects used by the same thread from the same memory region, cache hit rates may improve.To
implement these optimizations, each subdivision of the heap has thread-local variables that
track what parts of the thread-local heap are in use, what parts are free,and so on.
Then, the code that allocates new memory is written to use these thread-local data structures
and only take memory from the shared heap if the local heap is empty.
-----------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------
Thread Life Cycle. What states does a thread go through between initialization and completion?

INITï¼šThread creeation puts a thread into its INIT state and allocates and initializes 
per-thread data structures. Once that is done, thread creation code puts the thread into the
READY state by adding the thread to the ready list. In practice, the ready list is not in fact
a "list"; the operating system typically uses a more sophisticated data structure to keep 
track of runnable threads, such as a priority queue.

READY: Its TCB is on the ready list, and the values of its registers are stored in its TCB. At
any time, the scheduler can cause a thread to transition from READY to RUNNING by copying its
register values from its TCB to a processor's registers.

RUNNING: At this time, its register values are stored on the processor rathre than in the
TCB.A RUNNING thread can transition to the READY state in two ways:
	The scheduler can preempt a running thread and move it to the READY state by:
		1.saving the thread's registers to its TCB
		2.switching the processor to run the next thread on the ready list.
	A running thread can voluntarily relinquish the processor and go from RUNNING to READY by 
	calling yield (e.g.,thread_yield in the thread library).

WAITING: A thread in the WAITING state is waiting for some event. Whereas the scheduler can
move a thread in the READY state to the RUNNING state, a thread in the WAITING state cannot
run until some action by another moves it from WAITING to READY.After creating its children
threads, the main thread must wait for them complete, by calling thread_join once for each
child.If the specific child thread is not yet done at the time of the join, the main thread
goes from RUNNING to WAITING until the child thread exits.
While a thread waits for an event, it cannot make progress; therefore, it is not useful to run
it. Rather than continuing to run the thread or storing the TCB on the scheduler's ready list,
the TCB is stored on the waiting list of some synchronization variable associated with the 
event. When the required event occurs, the operating system moves the TCB from the
synchronization variable's waiting list to the scheduler's ready list, transitioning the thread
from WAITING to READY.

FINISHED: A thread in the FINISHED state never runs again. For example, the thread_exit call
lets a thread pass its exit value to its parent thread via thread_join.


Case: Where is my TCB?
A remarkably tricky implementation question is how to find the current thread's TCB. The thread
library needs access to the current TCB for a number of resons, e.g.,to change its priority or
to access thread-local variables.
The stack pointer is always unique to each thread. The thread library can store a pointer to
the thread TCB at the very bottom of the stack, underneath the procedure frames. (Some system
take this one step farther,and put the entire TCB at the bottom of the stack.) As long as
thread stacks are aligned to start at a fixed block boundary,the low order bits of the current
stack pointer can be masked to locate the pointer to the current TCB.
-----------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------
Implementing Kernel Threads. How do we implement the thread abstraction inside the operating
system kernel?

Kernel threads. A kernel thread executes kernel code and modifies kernel data structures.

Kernel threads and single-threaded processes. these processes can invoke system calls that run
concurrently with kernel threads inside the kernel.

Multi-threaded processes using kernel threads. Most operating systems provide a set of library
routines and system calls to allow applications to use multiple threads within a single
user-level process.

User-level threads.

Creating a Thread:
	
void thread_create(thread_t *thread, void (*func)(int), int arg){
	TCB *tcb = new TCB();
	thread->tcb = tcb;
	tcb->stack_size = INITIAL_STACK_SIZE;
	tcb->stack = new Stack(INITIAL_STACK_SIZE);

	tcb->sp = tcb->stack + INITIAL_STACK_SIZE;
	tcb->pc = stub;

	*(tcb->sp) = arg;
	tcb->sp--;
	*(tcb->sp) = func;
	tcb->sp--;

	thread_dummySwitchFrame(tcb);

	tcb->state = READY;
	readyList.add(tcb);
}

void stub(void (*func)(int), int arg){
	(*func) (arg);
	thread_exit(0);
}

There are three steps to creating a thread:

1.Allocate per-thread state. The first step in the thread constructor is to allocate space
for the thread's per-thread state: the TCB and stack

2.Initialize per-thread state. To initialize the TCB, the thread constructor sets the new
thread's registers to what they need to be when the thread starts RUNNING. The constructor
starts the thread in a dummy function,stub,which in turn calls func. We need this extra step
in case the func procedure returns instead of calling thread_exit.
To start at beginning of stub, the thread constructor sets up the stack as if stub was just
called by normal code;
In addition,we also push a dummy stack frame for thread_switch onto the stack.

3.Put TCB on ready list. The last step in creating a thread is to set its state to READY and
put the new TCB on the ready list, enabling the thread to be scheduled.

Deleting a Thread
When a thread calls thread_exit, there are two steps to deleting the thread:
	Remove the thread from the ready list so that it will never run again.
	Free the per-thread state allocated for the thread.

If a thread removes itself from the ready list and frees its own per-thread state, then the
program may break. For example, if a thread removes itself from the ready list but an interrupt
occurs before the thread finishes de-allocating its state, there is a memory leak: that thread
will never resume to de-allocate its state.
There is a simple fix: a thread never deletes its own state. Instead, some other thread must
do it. On exit, the thread transitions to the FINISHED state, moves its TCB from the ready list
to a list of finished threads the scheduler should never run. The thread can then safely
switch to the next thread on the ready list. Once the finished thread is no longer running, it
is safe for some other thread to free the state of the thread.

Thread Context Switch
We need to answer several questions:
	What triggers a context switch?
	How does a voluntary context switch (e.g., a call to thread_yield) work?
	How does an involuntary context switch differ from a voluntary one?
	What thread should the scheduler choose to run next?

What Triggers a Kernel Thread Context Switch?
	A thread context switch can be triggered by either a voluntary call into the thread library
	,or an involuntary interrupt or processor exception.

		Voluntary. most thread libraries provide a thread_yield call that lets the currently
		running thread voluntarily give up the processor to the next thread on the ready list.
		Similarly, the thread_join and thread_exit calls suspend execution of the current
		thread and start running a different one.

		Involuntary. An interrupt or processor exception could invoke an interrupt handler.

When switching between two threads, we need to temporatily defer interrupts until the switch is
complete to avoid confusion.
A subtle inconsistency might arise. Suppose a low priority thread is about to voluntarily
switch to a high priority thread. It pulls the high priority thread off the ready list, and
at that precise moment, an interrupt occurs. Suppose the interrupt moves a medium priority
thread from WAITING to READY. Since it appears that switches to the new thread. The high
priority thread is in limbo!
		
Voluntary Kernel Thread Context Switch

void thread_switch(oldThreadTCB, newThradTCB){
	pushad;
	oldThreadTCB->sp = %esp;
	%esp = newThreadTCB->sp;
	popad;
	return;
}
void thread_yield(){
	TCB *chosenTCB, *finishedTCB;

	disableInterrupts();

	chosenTCB = readyList.getNextThread();
	if(chosenTCB ==NULL){
		//Nothing else to run
	}els{
		runningThread->state = ready;
		readyList.add(runningThread);
		thread_switch(runningThread, chosenTCB);
		runningThread->state = running;
	}

	while((finishedTCB = finishedList->getNextThread()) != NULL){
		delete finishedTCB->stack;
		delete finishedTCB;
	}

	enableInterrupts();
}

void thread_dummySwitchFrame(newThread){
	*(tcb->sp) = stub;
	tcb->sp--;
	tcb->sp -= SizeOfPopad;
}

To create a new thread, thread_create must set up the stack of the new thread to be as if had
suspended execution just before performing its first instruction.
Then, if the newly created thread is the next thread to run, a thread can call thread_yield,
switch to the newly create thread, switch to its stack pointer, pop the register values off
the stack, and "return" to the new thread, even though it had never calledd switch in the 
first place.

Involuntary Kernel Thread Context Switch.
In short, comparing a switch between kernel threads to what happents on a user-mode transfer:
(1) there is no need to switch modes (and therefore no need to switch stacks)
(2) the handler can resume any thread on the ready list rather than always resuming the thread
	or process that was just suspended.

To be compatible with x86 interrupt hardware, the software implementation of thread_switch
would simulate the hardware case, saving the return instruction pointer and eflags register
before calling pushad to save the general-purpose registers.After switching to the new stack,
it would call iret to resume the new thread, whether the new thread was suspended by a
hardware event or a software call.
-----------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------
Combining Kernel Threads and Single-Threaded User Processes. How do we extend the
implementation of kernel threads to support simple-threaded processes?

A system with both kernel threads and single-threaded user processes.
The PCB has information about the process's address space; when a context switch occurs from
one process to another, the operating system must change the virtual memory mappings as well
as the register state.
Since the PCB and TCB each represent one thread, the kernel's ready list can contain a mix
of PCBs for processes and TCBs for kernel threads.When the scheduler chooses the next thread to
run, it can pick either kind.A thread switch is nearly identical whether switching between
kernels or switching between a process's thread and a kernel thread.
We can resume a process in the kernel using thread_switch.

Entering the handler. When an interrupt or exception occurs, if the processor detects that it 
is already in kernel mode, it just pushes the instruction pointer and eflags registers (but 
not the stack pointer) onto the existing stack. On the other hand, if the hardware detects that
it is switching from user-mode to kernel-mode, then the processor also changes the stack
pointer to the base of the interrupt stack and pushes the original stack pointer along with
the instruction pointer and eflags registers onto the new stack.

Returning from the handler. When the iret instruction is called, it inspects both the current
eflags register and the value on the stack that it will use to restore the earlier eflags
register. If the mode bit is identical, then iret just pops the instruction pointer and 
eflags register and continues to use the current stack. On the other hand, if the mode bit
differs, then the iret instruction pops not only the instruction pointer and eflags register,
but also the saved stack pointer, thus switching the processor's stack pointer to the saved
one.
-----------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------
Implementing Multi-threaded Processes. How do we implement the thread abstraction for 
multi-threaded applications?

Implementing Multi-Threaded Processes Using Kernel Threads.
The simplest way to support multiple threads per process is to use the kernel thread
implementation we have already described.

When a user-level thread accesses the thread library to do the same things, it uses a system
call to ask the kernel to do the operation on its behalf.
A thread in a process has:
	A user-level stack for executing user code.
	A kernel interrupt stack for when this thread makes system calls,causes a processor
	exception, or is interrupted.
	A kernel TCB for saving and restoring the per-thread state.
To create a thread, the user library allocates a user-level stack for the thread and then does
a system call into the kernel. The kernel allocates a TCB and interrupt stack, and arranges
the state of the thread to start execution on the user-level stack at the beginning of the
requested procedure.
After creating the thread, the krenel puts the new thread on the ready list, to be scheduled
like any other thread, and returns unique identifier for the user program to use when referring
to the newly created thread(e.g.,for join).
Thread join,yield,and exit work the same way: by calling into the kernel to perform the
requested function.

Implementing User-Level Threads Without Kernel Support
It is also possible to implement threads as a library completely at user level, without any
operating system support. JVM implemented what were called green threads, a pure user-level
implementation of threads.
The basic idea is simple. The thread library instantiates all of its data structures within
the process: TCBs, the ready list, the finished list, and the waiting lists all are just data
structures in the process's address space. Then, calls to the thread library are just procedure
calls, akin to how the same functions are implemented within a multi-threaded kernel.
To the operating system kernel, a multi-threaded application using green threads appears to be
a normal, single-threaded process. The process as a whole can make system calls, be time-sliced
,etc, Unlike with kernel threads, when a process using green threads is time-sliced, the entire
process,including all of its threads, is suspended.
A limitation of green threads is that the operating system kernel is unaware of the state of 
the user-level ready list. If the application performs a system call that blocks watiting for
I/O, the kernel is unable to run a different user-level thread.Likewise, on a multiprocessor,
the kernel is unable to run the different threads running within a single process on different
processors.
To implement preemptive multi-threading for some process P:
G1. The user-level thread library makes a system call to register a timer signal handler and
signal stack with the kernel.
G2. When a hardware timer interrupt occurs, the hardware saves P's register state and runs the
kernel's handler.
G3. Instead of restoring P's register state and resuming P where it was interrupted, the
kernel's handler copies P's saved registers onto P's signal stack.
G4. The kernel resumes execution in P at the registered signal handler on the signal stack.
G5. The signal handler copies the processor state of the preempted user-level thread from the
signal stack to that thread's TCB.
G6. The signal handler chooses the next thread to run, re-enables the signal handler(the 
equivalent of re-enabling interrupts), and restores the new thread's state from its TCB into
the processor. execution with the state(newly) stored on the signal stack.

Implementing User-Level Threads With Kernel Support
Various system take more of a hybrid model, attempting to combine the lightweight performance
and application control over scheduling found in user-level threads, while keeping many of the
advantages of kernel threads.
Hybrid Thread join.
	Thread libraries can avoid transitioning to the kernel in certain cases.
	Rather than always making a system call for thread_join to wait for target thread to finish
	, thread_exit can store its exit value in a data structure in the process's address space.
	Then,if the call to thread_join happens after the targeted thread has exited, it can
	immediately return the value without having to make a system call. However, if the call to
	thread_join precedes the call to thread_exit, then a system call is needed to transition to
	the WAITING state and let some other thread run.
Per-Processor Kernel Threads.
	It is possible to adapt the green threads approach to work on a multiprocessor. For many
	parallel scientific applications, the cost of creating and synchronizing threads is 
	paramount,and so an approach that requires a kernel call for most thread operations
	would be prohibitive.Instead,the library multiplexes user-level threads on top of kernel
	threads,in exactly the same way that the kernel multiplexes kernel threads on top of 
	physical processors.
	When the application starts up,the user-level thread library creates one kernel thread
	for each processor on the host machine.As long as there is no other activity on the system,
	the kernel will assign each of these threads a processor. Each kernel thread executes the
	user-level scheduler in parallel:pull the next thread off the user-level ready list,and 
	run it. Because thread scheduling decisions occur at user level, they can be flexible 
	and application-specific; for example, in a parallel graph algorithm, the programmer might
	adjust the priority of various threads based on the results of the computation on other
	part of the graph.
	most of the downsides of green threads are still present in these systems:
		Any time a user-level thread calls into the kernel, its host kernel thread blocks.
		This prevents the thread library from running a different user-level thread on that
		processor in the meantime.
		Any time the kernel time-slices a kernel thread, the user-level thread it was running
		is also suspended. The library cannot resume that thread until the kernel thread 
		resumes.
Scheduler Activations.
	In this approach, the user-level thread scheduler is notified for every kernel event that
	might affect the user-level thread system.For example, if one thread blocks in a system
	call, the activateion informs the user-level scheduler that it should choose anothe thread
	to run on that processor. Scheduler activations are like upcalls or signals,except that
	they do not return to the kernel; instead, they directly perform user-level thread suspend
	and resume.
	Various operations trigger a scheduler activation upcall:
		1.Increasing the number of virtual processors.
			When a program starts, it receives an activation to inform the program that it
			has been assigned a virtual processor:that activation runs the main thread and
			any other threads that might be created.To assign another virtual processor to
			the program, the kernel makes another activation upcall on the new processor;
			the user-level scheduler can pull a waiting therad off the ready list and run
			it.
		2.Dereasing the number of virtual processors.
			When the kernel preempts a virtual processor (e.g.,to give the processor to a
			different process), the kernel makes an upcall on one of the other processors
			assigned to the parallel program. The thread system can then move the preempted
			user-level thread onto the ready list, so that a different processor can run it.
		3.Transition to WAITING.
			When a user-level thread blocks in the krenel waiting for I/O, the kernel similarly
			makes an upcall to notify the user-level scheduler that it needs to take action,e.g.,
			to choose another thread to run while waiting for the I/O to complete.
		4.Transition from WAITING to READY.
			When the I/O completes, the kernel makes an upcall to notify the scheduler that the
			suspended thread can be resumed.
		5.Transition from RUNNING to idle.
			When a user-level activation finds an empty ready list (i.e., it has no more work 
			to do), it can make a system call into the kernel to return the virtual processor
			for use by some other process.
-----------------------------------------------------------------------------------------------



-----------------------------------------------------------------------------------------------
Alternative Abstractions. What other abstractions can we use to express and implement
concurrency?

Asynchronous I/O and event-driven programming. Asynchronous I/O and events allow a 
single-threaded program to cope with high-latency I/O devices by overlapping I/O with
processing and other I/O.
Asynchronous I/O is a way to allow a single-threaded process to issue multiple concurrent
I/O requests at the same time.The process makes a system call to issue an I/O request but
the call returns immediately, without waiting for the result.At a later time, the operating
system provides the result to the process by either:
	(1) calling a signal handler
	(2) placing the result in a queue in the process's memory
	(3) storing the result in kernel memory until the process makes another system call to 
		retrieve it.
The server does a select call that blocks until any of the 10 network connections has data
available to read. When the select call returns, it provides a list of connection with 
available to read. The thread can then read from those connections, knowing that the read 
will always return immediately. After processing the data, the thread then calls select 
again to wait for the next data to arrive.
Asynchronous I/O allows progress by many concurrent operating system requests. This 
approach gives rise to an event-driven programming pattern where a thread spins in a loop;
each iteration gets and processes the next I/O event.To process each event, the thread
typically maintains for each task a continuation,a data structure that keeps track of a task's
current state and next step. For example, handling a web request can involve a series of I/O
steps:
	(a) make a network connection
	(b) read a request from the network connection
	(c) read the requested data from disk
	(d) write the requested data to the network connection.
If a single thread is handling requests from multiple different clients at once,it must keep
track of where it is in that sequence for each client.

Event-Driven Programming vs. Threads
The differences are:
(1) whether the state is stored in a continuation or TCB
(2) whether the state save/restore is done explicitly by the application or automatically by
	the thread system.
-----------------------------------------------------------------------------------------------
// Event-driven
Hashtable<Buffer*> *hash;
while(1){
	connection = use select() to find a readable connection ID
	buffer = hash.remove(connection);
	got = read(connection, tmpBuf, TMP_SIZE);
	buffer -> append(tmpBuf, got);
	buffer = hash.put(connection, buffer);
}

// Thread-per-client
Buffer *b;
while(1){
	got = read(connection, tmpBuf, TMP_SIZE);
	buffer->apend(tmpBuf,got);
}

With events, the code uses select to determine which connection's packet to retrieve next.
With threads, the kernel transparently schedules each thread when data has arrived for it.
The state in both cases is also similar. In the event-driven case, the application maintains
a hash table containing the buffer state for each client.
The server must do a lookup to find the buffer each time a packet arrives for a particular
client.
In the thread-per-client case, each thread has just one buffer, and the operating system keeps
track of the different threads's states.
-----------------------------------------------------------------------------------------------
Performance: Coping with high-latency I/O devices.
The common wisdom has been that the event-driven approach was significantly faster for two 
reasons.
First, the space and context switch overheads of this approach could be lower because a thread
system must use generic code that allocates a stack for each thread's state and that saves
and restores all registers on each context switch, while the event-driven approach lets
programmers allocate and save/restore just the state needed for each task.
Second, some past operating systems had inefficient or unscalable implementations of their
thread systems, making it important not to create too many threads for each process.

Performance: Exploiting multiple processors.






Data parallel programming. With data parallel programming, all processors perform the same 
instructions in parallel on different parts of a data set.
-----------------------------------------------------------------------------------------------
